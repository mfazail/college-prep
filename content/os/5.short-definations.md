---
title: 'Short Definations'
---

#### Short Definations

**1. Process control block(PCB)**

Process Control Block is a data structure that contains information of the process related to it. The process control block is also known as a task control block, entry of the process table, etc.

It is very important for process management as the data structuring for processes is done in terms of the PCB. It also defines the current state of the operating system.

**2. Thread and its lifecycle** `repeated`

A process is divided into several light-weight processes, each light-weight process is said to be a thread.

_Life Cycle of Thread:_
1. **Born State:** A thread that has just been created.
2. **Ready State**: The thread is waiting for the processor (CPU).
3. **Running**: The System assigns the processor to the thread means that the thread is being executed.
4. **Blocked State**: The thread is waiting for an event to occur or waiting for an I/O device.
5. **Sleep**: A sleeping thread becomes ready after the designated sleep time expires.
6. **Dead**: The execution of the thread is finished.

**3. Critical Section and Critical Section Problem** `repeated`

**_Critical Section_** is the part of a program which tries to access shared resources. That resource may be any resource in a computer like a memory location, Data structure, CPU or any IO device.

The critical section cannot be executed by more than one process at the same time; operating system faces the difficulties in allowing and disallowing the processes from entering the critical section.

_Solution to critical problem solving must satify these condition:_

1. **Mutual Exclusion:** if one process is executing inside critical section then the other process must not enter in the critical section.
2. **Progress:** if one process doesn't need to execute into critical section then it should not stop other processes to get into the critical section.
3. **Bounded Waiting:** We should be able to predict the waiting time for every process to get into the critical section. The process must not be endlessly waiting for getting into the critical section.

**4. Demand Paging** `repeated`

The process recide in secondary memory and pages are loaded only on demand not in advance is known as demand paging.

**5. Page fault**

If the referred page is not present in the main memory then there will be a miss and the concept is called Page miss or page fault.

**6. Trashing** `repeated`

The Phenomenon of excessive movement of pages between main memory and secondary storage is known as "Trashing".

**7. Multilevel Queue Scheduling**

In multilevel queue scheduling, all the processes are assigned permanently to the queue at the time of entry. Processes will not move between queues and it may happen that the processes in the queue can be divided into different classes where classes have their own scheduling.

**8. Multilevel Feedback Queue Scheduling**

Multilevel Feedback Queue Scheduling (MLFQ) CPU Scheduling is like Multilevel Queue(MLQ) Scheduling but in this processes can move between the queues.

**9. Messages passing system**

Message Passing provides a mechanism to allow processes to communicate and to synchronize their actions without sharing the same address space.

**10. Shared memory system**

Shared memory system is the fundamental model of inter process communication. In a shared memory system, in the address space region the cooperating communicate with each other by establishing the shared memory region.

**11. Belady' anomaly**

Bélády's anomaly is the phenomenon in which increasing the number of page frames results in an increase in the number of page faults for certain memory access patterns. This phenomenon is commonly experienced when using the first-in first-out (FIFO) page replacement algorithm.

**12. Race condition** `repeated`

A race condition occurs when two threads access a shared variable at the same time. The first thread reads the variable, and the second thread reads the same value from the variable.

**13. Bankers Algorithm**

The algorithm, which can be used to avoid deadlock in such system, which is less efficient than Resource Allocation Graph algorithm, is Bankers Algorithm.

_Data structure required:_

1.**Need:** An n x m matrix indicates the remaining resource need of each process.  
2.**Allocation:** An n x m matrix defines the number of resources of each type currently allocated to each process.

3.**Max:** An n x m matrix defines maximum demand of each process.  
4.**Available:** An vector of length m indicates the number of available resources of each type.

**14. DeadLock**

Deadlock is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process. 

**15. Semaphore**

A semaphore is simply an integer variable that is shared between threads. This variable is used to solve the critical section problem and to achieve process synchronization in the multiprocessing environment.